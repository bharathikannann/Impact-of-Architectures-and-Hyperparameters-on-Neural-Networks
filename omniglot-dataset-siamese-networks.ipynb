{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from numpy import random\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.layers import Input, Lambda, Conv2D, MaxPooling2D, BatchNormalization, Dense, Flatten, Activation, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 20:03:30.506914: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2022-07-14 20:03:30.545847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2022-07-14 20:03:30.545890: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.10.2\n",
      "2022-07-14 20:03:31.896106: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.10\n",
      "2022-07-14 20:03:31.896193: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.10\n",
      "2022-07-14 20:03:32.460145: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2022-07-14 20:03:32.703019: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2022-07-14 20:03:34.217244: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10\n",
      "2022-07-14 20:03:34.370844: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.10\n",
      "2022-07-14 20:03:36.690674: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-07-14 20:03:36.691297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data loader\n",
    "Getting pairs of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(path):\n",
    "    X = list()\n",
    "    y = list()\n",
    "    # all the characters in an alphabet is a single class\n",
    "    y_out = 0\n",
    "    # stores info about the structure of characters in X and y\n",
    "    data_struct = dict()\n",
    "    # going thouugh all alphabets\n",
    "    for a in sorted(os.listdir(path)):\n",
    "        data_struct[a] = [y_out,None]\n",
    "        char_path = os.path.join(path,a)\n",
    "        # going through all characters\n",
    "        for char in sorted(os.listdir(char_path)):\n",
    "            char_images= []\n",
    "            # Going through all images in a character\n",
    "            for image in sorted(os.listdir(os.path.join(char_path,char))):\n",
    "                image_path = os.path.join(char_path,char,image)\n",
    "                char_images.append(cv2.cvtColor(cv2.imread(image_path),cv2.COLOR_BGR2GRAY))\n",
    "                y.append(y_out)\n",
    "            data_struct[a][1] = y_out\n",
    "            y_out += 1\n",
    "            X.append(char_images)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y, data_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_directory = \"siamese_networks/omniglot/images_background\"\n",
    "val_directory = \"siamese_networks/omniglot/images_evaluation\"\n",
    "X_train,y_train, X_train_data_struct = data_loader(train_directory)\n",
    "X_val,y_val, X_val_data_struct = data_loader(val_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((964, 20, 105, 105), (19280,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get batches of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size,data_type='train'):\n",
    "    if data_type == 'train':\n",
    "        X = X_train\n",
    "    elif data_type == 'val':\n",
    "        X = X_val\n",
    "    n_data, n_samples, w, h = X.shape\n",
    "    random_from_data = random.choice(n_data, size=batch_size, replace=False)\n",
    "    y = np.zeros((batch_size,))\n",
    "    y[batch_size//2:] = 1\n",
    "    pairs = [np.zeros((batch_size,w,h,1)) for _ in range(2)]\n",
    "    for i in range(batch_size):\n",
    "        random_sample = random.randint(n_samples)\n",
    "        pairs[0][i,:,:,:] = X[random_from_data[i],random_sample,:,:].reshape(w,h,1)\n",
    "        random_from_data_2 = 0\n",
    "        if i >= batch_size // 2:\n",
    "            random_from_data_2 = random_from_data[i]\n",
    "        else:\n",
    "            random_from_data_2 = (random_from_data[i] + random.randint(1,n_data)) % n_data\n",
    "        random_sample_2 = random.randint(n_samples)\n",
    "        pairs[1][i,:,:,:] = X[random_from_data_2,random_sample_2,:,:].reshape(w,h,1)\n",
    "    return pairs,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make N shot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_shot_prediction(N,data_type='val'):\n",
    "    if data_type == 'train':\n",
    "        X = X_train\n",
    "    elif data_type == 'val':\n",
    "        X = X_val\n",
    "    n_data, n_samples, w, h = X.shape\n",
    "    random_data = random.choice(n_data,size=(N,))\n",
    "    random_sample = random.choice(n_samples,size=(N,))\n",
    "    \n",
    "    query = np.array([X[random_data[0],random.randint(n_samples)]]*N).reshape(N,w,h,1)\n",
    "    support_set = X[random_data,random_sample].reshape(N,w,h,1)\n",
    "    targets = np.zeros((N,))\n",
    "    targets[0] = 1\n",
    "    \n",
    "    query,support_set,targets = shuffle(query,support_set,targets)\n",
    "    \n",
    "    return [query,support_set], targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_shot(model,N,k,data_type='val'):\n",
    "    n_correct = 0\n",
    "    for _ in range(k):\n",
    "        X, y = one_shot_prediction(N,data_type)\n",
    "        preds = model.predict(X)\n",
    "        if np.argmax(y) == np.argmax(preds):\n",
    "            n_correct += 1\n",
    "    return round(((n_correct / k) * 100),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siamese_model(image_shape, droupout_rate):\n",
    "    left_image = Input(image_shape)\n",
    "    right_image = Input(image_shape)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32,(5,5),input_shape=image_shape,activation='relu',kernel_regularizer='l2'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2,strides=(2,2)))\n",
    "    model.add(Dropout(droupout_rate))\n",
    "\n",
    "    model.add(Conv2D(64,(5,5),kernel_regularizer='l2'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2,strides=(2,2)))\n",
    "    model.add(Dropout(droupout_rate))\n",
    "\n",
    "    model.add(Conv2D(128,(5,5),kernel_regularizer='l2'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2,strides=(2,2)))\n",
    "    model.add(Dropout(droupout_rate))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # ----------------------------------------------------------------\n",
    "    # Conversion to MLP\n",
    "    # model.add(Dense(2048,activation='relu',kernel_regularizer='l2'))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(Dropout(droupout_rate))\n",
    "    # model.add(Dense(1024,activation='relu',kernel_regularizer='l2'))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(Dropout(droupout_rate))\n",
    "    # model.add(Dense(512,activation='relu',kernel_regularizer='l2'))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(Dropout(droupout_rate))\n",
    "\n",
    "    model.add(Dense(256,activation='sigmoid',kernel_regularizer='l2'))\n",
    "\n",
    "    left_embeddings = model(left_image)\n",
    "    right_embeddings = model(right_image)\n",
    "\n",
    "    L1_Layer = Lambda(lambda tensors: K.abs(tensors[0] - tensors[1]))\n",
    "    L1_Dist = L1_Layer([left_embeddings,right_embeddings])\n",
    "    out = Dense(1,activation='sigmoid',kernel_regularizer='l2')(L1_Dist)\n",
    "\n",
    "    siamese_model = Model(inputs=[left_image,right_image],outputs=out)\n",
    "\n",
    "    return siamese_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 20:07:49.368774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: Tesla V100-PCIE-16GB computeCapability: 7.0\n",
      "coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s\n",
      "2022-07-14 20:07:49.369123: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
      "2022-07-14 20:07:49.389146: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.10.2\n",
      "2022-07-14 20:08:06.463772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-07-14 20:08:06.463813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
      "2022-07-14 20:08:06.463823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
      "2022-07-14 20:08:06.464935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14683 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-16GB, pci bus id: 0000:01:00.0, compute capability: 7.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 105, 105, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 105, 105, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 256)          2911488     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 256)          0           sequential[0][0]                 \n",
      "                                                                 sequential[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            257         lambda[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,911,745\n",
      "Trainable params: 2,911,745\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "droupout_rate = 0.3\n",
    "initial_learning_rate = 0.05\n",
    "decay_steps=4000\n",
    "decay_rate=0.0001\n",
    "\n",
    "n_data, n_samples, w, h = X_train.shape\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "# initializer = tf.keras.initializers.Zeros()\n",
    "model = siamese_model((w, h, 1), droupout_rate)\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 250\n",
    "N_way = 20\n",
    "n_iterations = 5000\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "curr_time = time.time()\n",
    "for i in range(0,n_iterations+1):\n",
    "    x,y = get_batch(batch_size)\n",
    "    loss = model.train_on_batch(x,y)\n",
    "    if i % 100 == 0:\n",
    "        losses.append(loss[0])\n",
    "        train_acc_temp = test_one_shot(model,N_way,trials,'train')\n",
    "        val_acc_temp = test_one_shot(model,N_way,trials,'val')\n",
    "        train_accuracy.append(train_acc_temp)\n",
    "        val_accuracy.append(val_acc_temp)\n",
    "        print('Iteration',i,'('+str(round(time.time() - curr_time,1))+'s) - Loss:',loss[0],'Accuracy:',round(loss[1],2),'',end='')\n",
    "        print('train accuracy:', train_acc_temp,'%, ',end='')\n",
    "        print('val accuracy:', val_acc_temp,'%')\n",
    "        curr_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_range = [i*10 for i in range(1, len(train_accuracy) + 1)]\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.ylim(0, 90)\n",
    "\n",
    "plt.plot(epochs_range, train_accuracy, label='Train Set')\n",
    "plt.plot(epochs_range, val_accuracy, label='Val Set')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "plt.plot([i for i in range(len(losses))], losses, label='Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 1\n",
    "additional_info = \"\"\n",
    "model_name = \"my_model_v\"+str(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = {}\n",
    "model_info[\"version\"] = 1\n",
    "model_info[\"trials\"] = str(trials)\n",
    "model_info[\"N way\"] = str(N_way)\n",
    "model_info[\"Batch size\"] = str(batch_size)\n",
    "model_info[\"N iterations\"] = str(n_iterations)\n",
    "model_info[\"drop out\"] = str(droupout_rate)\n",
    "model_info[\"initial_learning_rate\"] = str(initial_learning_rate)\n",
    "model_info[\"decay_steps\"] = str(decay_steps)\n",
    "model_info[\"decay_rate\"] = str(decay_rate)\n",
    "model_info[\"Additional info\"] = additional_info\n",
    "model_info[\"losses\"] = str(losses)\n",
    "model_info[\"Training_accuracy\"] = str(train_accuracy)\n",
    "model_info[\"Validation_accuracy\"] = str(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True\n",
    "if save:\n",
    "    model.save(\"siamese_networks/models/\" + model_name)\n",
    "    np.savetxt(\"siamese_networks/models/\" + model_name+\"/losses.csv\", losses,delimiter =\", \", fmt ='%f') \n",
    "    np.savetxt(\"siamese_networks/models/\" + model_name+\"/train_accuracy.csv\", train_accuracy,delimiter =\", \", fmt ='%f') \n",
    "    np.savetxt(\"siamese_networks/models/\" + model_name+\"/val_accuracy.csv\", val_accuracy,delimiter =\", \", fmt ='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"siamese_networks/models/\" + model_name + \"/info.json\", \"w\") as outfile:\n",
    "    json.dump(model_info, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensoflow2 via SLURM (gpu-node)",
   "language": "python",
   "name": "jupyter-eg-kernel-slurm-tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
